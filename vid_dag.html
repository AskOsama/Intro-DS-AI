<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Video Processing Pipeline DAG</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 40px 20px;
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        h1 {
            color: white;
            margin-bottom: 10px;
            text-align: center;
            font-size: 2.5em;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }

        .subtitle {
            color: rgba(255,255,255,0.85);
            margin-bottom: 30px;
            text-align: center;
            font-size: 1.1em;
        }

        .dag-container {
            background: white;
            border-radius: 20px;
            padding: 50px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            max-width: 1400px;
            width: 100%;
            position: relative;
        }

        .pipeline {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 150px 60px;
            position: relative;
            margin: 40px 0;
        }

        .node {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 12px;
            text-align: center;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
            position: relative;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            cursor: pointer;
            min-height: 120px;
            display: flex;
            flex-direction: column;
            justify-content: center;
            z-index: 10;
        }

        .node:hover {
            transform: translateY(-5px) scale(1.05);
            box-shadow: 0 8px 25px rgba(0,0,0,0.3);
        }

        .node-id {
            font-weight: bold;
            font-size: 1.5em;
            margin-bottom: 8px;
            color: #ffd700;
        }

        .node-label {
            font-size: 0.95em;
            line-height: 1.4;
            font-weight: 600;
        }

        .node-desc {
            font-size: 0.75em;
            margin-top: 6px;
            opacity: 0.9;
            line-height: 1.3;
        }

        /* Audio Processing Nodes */
        .node.audio {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        }

        /* Visual Processing Nodes */
        .node.visual {
            background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        }

        /* Merge/Output Nodes */
        .node.merge {
            background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
            color: #1a1a2e;
        }

        .node.merge .node-id {
            color: #065f46;
        }

        /* Source Node */
        .node.source {
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
            color: #1a1a2e;
        }

        .node.source .node-id {
            color: #7c2d12;
        }

        /* Grid positioning */
        .node-A { grid-column: 3; grid-row: 1; }
        .node-B { grid-column: 2; grid-row: 3; }
        .node-C { grid-column: 4; grid-row: 3; }
        .node-D { grid-column: 2; grid-row: 4; }
        .node-E { grid-column: 4; grid-row: 4; }
        .node-F { grid-column: 2; grid-row: 5; }
        .node-G { grid-column: 4; grid-row: 5; }
        .node-H { grid-column: 2; grid-row: 6; }
        .node-I { grid-column: 4; grid-row: 6; }
        .node-J { grid-column: 3; grid-row: 7; }
        .node-K { grid-column: 3; grid-row: 8; }

        /* SVG for arrows */
        .arrows-svg {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none;
            z-index: 1;
        }

        .arrow-line {
            stroke: #9333ea;
            stroke-width: 3;
            fill: none;
            marker-end: url(#arrowhead);
        }

        .arrow-line.audio {
            stroke: #f5576c;
        }

        .arrow-line.visual {
            stroke: #00b4d8;
        }

        .arrow-line.merge {
            stroke: #22c55e;
        }

        .arrow-line.source {
            stroke: #f97316;
        }

        .edge-label {
            position: absolute;
            background: rgba(255,255,255,0.95);
            color: #374151;
            padding: 4px 8px;
            border-radius: 6px;
            font-size: 0.7em;
            white-space: nowrap;
            box-shadow: 0 2px 6px rgba(0,0,0,0.15);
            z-index: 5;
            border: 1px solid #e5e7eb;
        }

        .legend {
            display: flex;
            gap: 30px;
            justify-content: center;
            margin-top: 40px;
            flex-wrap: wrap;
        }

        .legend-item {
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .legend-color {
            width: 40px;
            height: 25px;
            border-radius: 6px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.2);
        }

        .legend-color.source {
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
        }

        .legend-color.audio {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        }

        .legend-color.visual {
            background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        }

        .legend-color.merge {
            background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
        }

        .legend-text {
            font-size: 14px;
            color: #374151;
            font-weight: 500;
        }

        /* Technical Section Styles */
        .tech-section {
            margin-top: 50px;
            padding-top: 40px;
            border-top: 2px solid #e5e7eb;
        }

        .tech-section h2 {
            text-align: center;
            color: #1f2937;
            margin-bottom: 30px;
            font-size: 1.8em;
        }

        .func-card {
            background: #fff;
            border-radius: 12px;
            margin-bottom: 25px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            border-left: 5px solid #667eea;
        }

        .func-card.source {
            border-left-color: #f97316;
        }

        .func-card.audio {
            border-left-color: #ec4899;
        }

        .func-card.visual {
            border-left-color: #06b6d4;
        }

        .func-card.merge {
            border-left-color: #22c55e;
        }

        .func-header {
            background: #f9fafb;
            padding: 15px 20px;
            display: flex;
            align-items: center;
            gap: 15px;
            border-bottom: 1px solid #e5e7eb;
        }

        .func-id {
            background: #667eea;
            color: white;
            width: 36px;
            height: 36px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            font-size: 1.1em;
        }

        .func-card.source .func-id {
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
            color: #7c2d12;
        }

        .func-card.audio .func-id {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        }

        .func-card.visual .func-id {
            background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        }

        .func-card.merge .func-id {
            background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
            color: #065f46;
        }

        .func-name {
            font-weight: 600;
            font-size: 1.2em;
            color: #1f2937;
            flex-grow: 1;
        }

        .func-type {
            background: #e5e7eb;
            color: #4b5563;
            padding: 4px 12px;
            border-radius: 20px;
            font-size: 0.8em;
            font-weight: 500;
        }

        .func-body {
            padding: 20px;
        }

        .func-desc {
            color: #4b5563;
            line-height: 1.8;
        }

        .func-desc p {
            margin-bottom: 10px;
        }

        .func-desc strong {
            color: #1f2937;
        }

        .branch-label {
            position: absolute;
            font-weight: 700;
            font-size: 0.9em;
            text-transform: uppercase;
            letter-spacing: 2px;
            color: #6b7280;
            z-index: 2;
        }

        .branch-label.audio {
            color: #db2777;
        }

        .branch-label.visual {
            color: #0891b2;
        }

        /* Edge tuple list */
        .edge-list {
            margin-top: 40px;
            padding: 20px;
            background: #f9fafb;
            border-radius: 12px;
            border: 1px solid #e5e7eb;
        }

        .edge-list h3 {
            margin-bottom: 15px;
            color: #374151;
        }

        .edge-list pre {
            font-family: 'Courier New', monospace;
            font-size: 0.85em;
            color: #4b5563;
            line-height: 1.6;
        }

        @media (max-width: 1200px) {
            .pipeline {
                gap: 120px 40px;
            }
            
            .dag-container {
                padding: 30px;
            }
        }

        /* Language Switcher */
        .language-switcher {
            position: fixed;
            top: 20px;
            right: 20px;
            z-index: 1000;
        }

        .lang-button {
            background: rgba(255, 255, 255, 0.9);
            color: #667eea;
            padding: 8px 16px;
            border-radius: 20px;
            text-decoration: none;
            font-weight: 600;
            font-size: 14px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.2);
            transition: all 0.3s ease;
        }

        .lang-button:hover {
            background: white;
            transform: translateY(-2px);
            box-shadow: 0 4px 15px rgba(0,0,0,0.3);
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 1.8em;
            }

            .pipeline {
                grid-template-columns: repeat(3, 1fr);
                gap: 80px 20px;
            }

            .node-A { grid-column: 2; grid-row: 1; }
            .node-B { grid-column: 1; grid-row: 3; }
            .node-C { grid-column: 3; grid-row: 3; }
            .node-D { grid-column: 1; grid-row: 4; }
            .node-E { grid-column: 3; grid-row: 4; }
            .node-F { grid-column: 1; grid-row: 5; }
            .node-G { grid-column: 3; grid-row: 5; }
            .node-H { grid-column: 1; grid-row: 6; }
            .node-I { grid-column: 3; grid-row: 6; }
            .node-J { grid-column: 2; grid-row: 7; }
            .node-K { grid-column: 2; grid-row: 8; }

            .dag-container {
                padding: 20px;
            }

            .edge-label {
                display: none;
            }
        }
    </style>
</head>
<body>
    <!-- Language Switcher -->
    <div class="language-switcher">
        <a href="vid_dag_ar.html" class="lang-button" title="Switch to Arabic">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
    </div>

    <h1>Video Analysis Pipeline DAG</h1>
    <p class="subtitle">Parallel AI Inference Architecture using MapReduce/DAG Principles</p>
    
    <div class="dag-container">
        <!-- Branch Labels -->
        <div class="branch-label audio" style="top: 280px; left: 80px;">Audio Branch</div>
        <div class="branch-label visual" style="top: 280px; right: 80px;">Video Branch</div>

        <div class="pipeline">
            <!-- Source Node -->
            <div class="node source node-A" data-node="A">
                <div class="node-id">A</div>
                <div class="node-label">Raw Videos</div>
                <div class="node-desc">Batch of video files to process</div>
            </div>

            <!-- Audio Branch -->
            <div class="node audio node-B" data-node="B">
                <div class="node-id">B</div>
                <div class="node-label">Extract Audio</div>
                <div class="node-desc">Separate audio stream using FFmpeg</div>
            </div>

            <div class="node audio node-D" data-node="D">
                <div class="node-id">D</div>
                <div class="node-label">Separate Speakers</div>
                <div class="node-desc">Cocktail party problem solver<br>(Conv-TasNet, SepFormer)</div>
            </div>

            <div class="node audio node-F" data-node="F">
                <div class="node-id">F</div>
                <div class="node-label">Speech to Text</div>
                <div class="node-desc">Whisper ASR transcription</div>
            </div>

            <div class="node audio node-H" data-node="H">
                <div class="node-id">H</div>
                <div class="node-label">Summarize Opinions</div>
                <div class="node-desc">LLM summarization per speaker<br>(Llama, GPT)</div>
            </div>

            <!-- Visual Branch -->
            <div class="node visual node-C" data-node="C">
                <div class="node-id">C</div>
                <div class="node-label">Extract Frames</div>
                <div class="node-desc">Sample frames using OpenCV</div>
            </div>

            <div class="node visual node-E" data-node="E">
                <div class="node-id">E</div>
                <div class="node-label">Detect People</div>
                <div class="node-desc">Person detection & localization<br>(YOLO, Faster R-CNN)</div>
            </div>

            <div class="node visual node-G" data-node="G">
                <div class="node-id">G</div>
                <div class="node-label">Describe People</div>
                <div class="node-desc">Vision-language model<br>(BLIP-2, LLaVA)</div>
            </div>

            <div class="node visual node-I" data-node="I">
                <div class="node-id">I</div>
                <div class="node-label">Collect Visual Info</div>
                <div class="node-desc">Aggregate person descriptions</div>
            </div>

            <!-- Merge Nodes -->
            <div class="node merge node-J" data-node="J">
                <div class="node-id">J</div>
                <div class="node-label">Combine All Results</div>
                <div class="node-desc">Correlate audio & visual using timestamps</div>
            </div>

            <div class="node merge node-K" data-node="K">
                <div class="node-id">K</div>
                <div class="node-label">Save Final Report</div>
                <div class="node-desc">Store in shared filesystem</div>
            </div>
        </div>

        <!-- Edge Labels (positioned via JS) -->
        <div class="edge-label" id="label-A-B">video files</div>
        <div class="edge-label" id="label-A-C">video files</div>
        <div class="edge-label" id="label-B-D">mixed audio</div>
        <div class="edge-label" id="label-C-E">frames + timestamps</div>
        <div class="edge-label" id="label-D-F">speaker audio tracks</div>
        <div class="edge-label" id="label-E-G">cropped person images</div>
        <div class="edge-label" id="label-F-H">transcripts + timestamps</div>
        <div class="edge-label" id="label-G-I">person descriptions</div>
        <div class="edge-label" id="label-H-J">opinion summaries</div>
        <div class="edge-label" id="label-I-J">visual descriptions</div>
        <div class="edge-label" id="label-J-K">final unified report</div>

        <svg class="arrows-svg" id="arrows">
            <defs>
                <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                    <polygon points="0 0, 10 3, 0 6" fill="#9333ea" />
                </marker>
                <marker id="arrowhead-audio" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                    <polygon points="0 0, 10 3, 0 6" fill="#f5576c" />
                </marker>
                <marker id="arrowhead-visual" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                    <polygon points="0 0, 10 3, 0 6" fill="#00b4d8" />
                </marker>
                <marker id="arrowhead-merge" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                    <polygon points="0 0, 10 3, 0 6" fill="#22c55e" />
                </marker>
                <marker id="arrowhead-source" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                    <polygon points="0 0, 10 3, 0 6" fill="#f97316" />
                </marker>
            </defs>
        </svg>

        <div class="legend">
            <div class="legend-item">
                <div class="legend-color source"></div>
                <div class="legend-text">Source Input</div>
            </div>
            <div class="legend-item">
                <div class="legend-color audio"></div>
                <div class="legend-text">Audio Processing</div>
            </div>
            <div class="legend-item">
                <div class="legend-color visual"></div>
                <div class="legend-text">Visual Processing</div>
            </div>
            <div class="legend-item">
                <div class="legend-color merge"></div>
                <div class="legend-text">Merge & Output</div>
            </div>
        </div>

        <!-- Edge Tuple List -->
        <div class="edge-list">
            <h3>üìä Edge List: (source, target, data_description)</h3>
            <pre>
(A, B, "video files")
(A, C, "video files")
(B, D, "mixed audio with overlapping voices")
(C, E, "extracted frames with timestamps")
(D, F, "isolated speaker audio tracks")
(E, G, "cropped person images with bounding boxes")
(F, H, "speaker transcripts with timestamps")
(G, I, "person descriptions with timestamps")
(H, J, "opinion summaries per speaker")
(I, J, "aggregated visual descriptions")
(J, K, "final unified multimodal report")
            </pre>
        </div>

        <!-- Technical Function Descriptions -->
        <div class="tech-section">
            <h2>‚öôÔ∏è Node Function Specifications</h2>

            <!-- Node A -->
            <div class="func-card source">
                <div class="func-header">
                    <span class="func-id">A</span>
                    <span class="func-name">Raw Videos</span>
                    <span class="func-type">Source</span>
                </div>
                <div class="func-body">
                    <div class="func-desc">
                        <p><strong>Purpose:</strong> Entry point that loads a batch of video files from shared storage for processing.</p>
                        <p><strong>Input:</strong> Directory path containing video files (.mp4, .avi, .mkv, .mov)</p>
                        <p><strong>Output:</strong> Collection of video files with metadata (duration, resolution, frame rate)</p>
                        <p><strong>Parallelism:</strong> Each video can be processed independently ‚Äî ideal for data-parallel distribution across workers.</p>
                    </div>
                </div>
            </div>

            <!-- Node B -->
            <div class="func-card audio">
                <div class="func-header">
                    <span class="func-id">B</span>
                    <span class="func-name">Extract Audio</span>
                    <span class="func-type">Transform</span>
                </div>
                <div class="func-body">
                    <div class="func-desc">
                        <p><strong>Purpose:</strong> Separates the audio track from each video file using FFmpeg demuxing.</p>
                        <p><strong>Input:</strong> Video file</p>
                        <p><strong>Output:</strong> Audio stream in WAV format (16kHz sample rate, mono channel for ASR compatibility)</p>
                        <p><strong>Tools:</strong> FFmpeg ‚Äî industry-standard multimedia framework for audio/video processing</p>
                        <p><strong>Parallelism:</strong> Stateless operation ‚Äî each video's audio extraction runs independently.</p>
                    </div>
                </div>
            </div>

            <!-- Node C -->
            <div class="func-card visual">
                <div class="func-header">
                    <span class="func-id">C</span>
                    <span class="func-name">Extract Frames</span>
                    <span class="func-type">Transform</span>
                </div>
                <div class="func-body">
                    <div class="func-desc">
                        <p><strong>Purpose:</strong> Samples frames from video at configurable intervals (e.g., 1 frame per second).</p>
                        <p><strong>Input:</strong> Video file + sampling rate (frames per second)</p>
                        <p><strong>Output:</strong> Collection of image frames with timestamps</p>
                        <p><strong>Tools:</strong> OpenCV ‚Äî open-source computer vision library</p>
                        <p><strong>Parallelism:</strong> Stateless operation ‚Äî each video's frame extraction runs independently.</p>
                    </div>
                </div>
            </div>

            <!-- Node D -->
            <div class="func-card audio">
                <div class="func-header">
                    <span class="func-id">D</span>
                    <span class="func-name">Separate Speakers</span>
                    <span class="func-type">AI Model</span>
                </div>
                <div class="func-body">
                    <div class="func-desc">
                        <p><strong>Purpose:</strong> Solves the "cocktail party problem" ‚Äî isolates individual speaker voices from mixed/overlapping audio using blind source separation.</p>
                        <p><strong>Input:</strong> Mixed audio stream with multiple overlapping speakers</p>
                        <p><strong>Output:</strong> Separate audio tracks, one per detected speaker</p>
                        <p><strong>AI Models:</strong> SepFormer, Conv-TasNet, DPRNN ‚Äî deep learning models trained on speech separation tasks</p>
                        <p><strong>Parallelism:</strong> GPU-intensive inference ‚Äî benefits from batching multiple audio files per GPU.</p>
                    </div>
                </div>
            </div>

            <!-- Node E -->
            <div class="func-card visual">
                <div class="func-header">
                    <span class="func-id">E</span>
                    <span class="func-name">Detect People</span>
                    <span class="func-type">AI Model</span>
                </div>
                <div class="func-body">
                    <div class="func-desc">
                        <p><strong>Purpose:</strong> Detects and localizes people in each frame, drawing bounding boxes and cropping detected individuals.</p>
                        <p><strong>Input:</strong> Image frame</p>
                        <p><strong>Output:</strong> List of cropped person images with bounding box coordinates and confidence scores</p>
                        <p><strong>AI Models:</strong> YOLOv8, Faster R-CNN, DETR ‚Äî real-time object detection models</p>
                        <p><strong>Parallelism:</strong> Highly parallelizable ‚Äî each frame processed independently; batch inference on GPU.</p>
                    </div>
                </div>
            </div>

            <!-- Node F -->
            <div class="func-card audio">
                <div class="func-header">
                    <span class="func-id">F</span>
                    <span class="func-name">Speech to Text</span>
                    <span class="func-type">AI Model</span>
                </div>
                <div class="func-body">
                    <div class="func-desc">
                        <p><strong>Purpose:</strong> Converts isolated speaker audio into text transcripts with word-level timestamps.</p>
                        <p><strong>Input:</strong> Single-speaker audio track</p>
                        <p><strong>Output:</strong> Transcript with text, timestamps, and word-level timing</p>
                        <p><strong>AI Models:</strong> OpenAI Whisper (base/medium/large), Faster-Whisper ‚Äî state-of-the-art automatic speech recognition</p>
                        <p><strong>Parallelism:</strong> Each speaker's audio transcribed independently ‚Äî scales linearly with GPU count.</p>
                    </div>
                </div>
            </div>

            <!-- Node G -->
            <div class="func-card visual">
                <div class="func-header">
                    <span class="func-id">G</span>
                    <span class="func-name">Describe People</span>
                    <span class="func-type">AI Model</span>
                </div>
                <div class="func-body">
                    <div class="func-desc">
                        <p><strong>Purpose:</strong> Generates natural language descriptions of detected individuals (appearance, clothing, actions, expressions).</p>
                        <p><strong>Input:</strong> Cropped person image</p>
                        <p><strong>Output:</strong> Text description of the person with timestamp</p>
                        <p><strong>AI Models:</strong> BLIP-2, LLaVA, InstructBLIP ‚Äî vision-language models that understand and describe images</p>
                        <p><strong>Parallelism:</strong> Each person crop processed independently ‚Äî ideal for GPU batching.</p>
                    </div>
                </div>
            </div>

            <!-- Node H -->
            <div class="func-card audio">
                <div class="func-header">
                    <span class="func-id">H</span>
                    <span class="func-name">Summarize Opinions</span>
                    <span class="func-type">AI Model</span>
                </div>
                <div class="func-body">
                    <div class="func-desc">
                        <p><strong>Purpose:</strong> Analyzes each speaker's transcript to extract key opinions, arguments, and main points with time references.</p>
                        <p><strong>Input:</strong> Speaker transcript with timestamps</p>
                        <p><strong>Output:</strong> Structured summary containing main opinions, key arguments, and time references</p>
                        <p><strong>AI Models:</strong> Llama-3, GPT-4, Mistral, Claude ‚Äî large language models for text understanding and summarization</p>
                        <p><strong>Parallelism:</strong> Each speaker's transcript summarized independently.</p>
                    </div>
                </div>
            </div>

            <!-- Node I -->
            <div class="func-card visual">
                <div class="func-header">
                    <span class="func-id">I</span>
                    <span class="func-name">Collect Visual Info</span>
                    <span class="func-type">Reducer</span>
                </div>
                <div class="func-body">
                    <div class="func-desc">
                        <p><strong>Purpose:</strong> Aggregates all person descriptions from the video branch, clusters them by individual identity, and builds appearance timelines.</p>
                        <p><strong>Input:</strong> All person descriptions from all frames</p>
                        <p><strong>Output:</strong> Visual summary with unique individuals, their descriptions, and when they appeared</p>
                        <p><strong>Processing:</strong> Clustering by time proximity and visual similarity to group the same person across frames</p>
                        <p><strong>Parallelism:</strong> Reducer node ‚Äî collects parallel outputs into single aggregated result.</p>
                    </div>
                </div>
            </div>

            <!-- Node J -->
            <div class="func-card merge">
                <div class="func-header">
                    <span class="func-id">J</span>
                    <span class="func-name">Combine All Results</span>
                    <span class="func-type">Reducer</span>
                </div>
                <div class="func-body">
                    <div class="func-desc">
                        <p><strong>Purpose:</strong> Merges audio and visual analysis by correlating speakers with visible individuals using temporal alignment.</p>
                        <p><strong>Input:</strong> Speaker summaries (from audio branch) + Visual summary (from video branch)</p>
                        <p><strong>Output:</strong> Combined analysis with speaker-to-person matching and confidence scores</p>
                        <p><strong>Processing:</strong> Temporal overlap analysis ‚Äî matches speakers to people visible during their speech segments</p>
                        <p><strong>Parallelism:</strong> Fan-in node ‚Äî synchronization barrier where both branches must complete before merging.</p>
                    </div>
                </div>
            </div>

            <!-- Node K -->
            <div class="func-card merge">
                <div class="func-header">
                    <span class="func-id">K</span>
                    <span class="func-name">Save Final Report</span>
                    <span class="func-type">Sink</span>
                </div>
                <div class="func-body">
                    <div class="func-desc">
                        <p><strong>Purpose:</strong> Serializes the combined analysis into structured file formats and saves to shared filesystem.</p>
                        <p><strong>Input:</strong> Combined analysis object</p>
                        <p><strong>Output:</strong> Report files (JSON for data, HTML/PDF for human-readable format) in shared storage</p>
                        <p><strong>Storage:</strong> Shared filesystem accessible by all pipeline workers and downstream consumers</p>
                        <p><strong>Parallelism:</strong> Sink node ‚Äî final output stage; one write per video batch.</p>
                    </div>
                </div>
            </div>

        </div>
    </div>

    <script>
        const edges = [
            {from: "A", to: "B", type: "source"},
            {from: "A", to: "C", type: "source"},
            {from: "B", to: "D", type: "audio"},
            {from: "C", to: "E", type: "visual"},
            {from: "D", to: "F", type: "audio"},
            {from: "E", to: "G", type: "visual"},
            {from: "F", to: "H", type: "audio"},
            {from: "G", to: "I", type: "visual"},
            {from: "H", to: "J", type: "merge"},
            {from: "I", to: "J", type: "merge"},
            {from: "J", to: "K", type: "merge"}
        ];

        function drawArrows() {
            const svg = document.getElementById('arrows');
            const container = document.querySelector('.dag-container');
            const containerRect = container.getBoundingClientRect();
            
            // Keep defs
            const defs = svg.querySelector('defs').outerHTML;
            svg.innerHTML = defs;

            edges.forEach(edge => {
                const fromNode = document.querySelector(`.node-${edge.from}`);
                const toNode = document.querySelector(`.node-${edge.to}`);
                
                if (!fromNode || !toNode) return;

                const fromRect = fromNode.getBoundingClientRect();
                const toRect = toNode.getBoundingClientRect();

                const x1 = fromRect.left - containerRect.left + fromRect.width / 2;
                const y1 = fromRect.bottom - containerRect.top;
                const x2 = toRect.left - containerRect.left + toRect.width / 2;
                const y2 = toRect.top - containerRect.top;

                const path = document.createElementNS('http://www.w3.org/2000/svg', 'path');
                
                const verticalDistance = y2 - y1;
                const controlPointOffset = Math.min(verticalDistance * 0.4, 60);
                
                const d = `M ${x1} ${y1} C ${x1} ${y1 + controlPointOffset}, ${x2} ${y2 - controlPointOffset}, ${x2} ${y2}`;
                path.setAttribute('d', d);
                path.setAttribute('class', `arrow-line ${edge.type}`);
                path.setAttribute('marker-end', `url(#arrowhead-${edge.type})`);
                
                svg.appendChild(path);

                // Position edge label
                const label = document.getElementById(`label-${edge.from}-${edge.to}`);
                if (label) {
                    const midX = (x1 + x2) / 2;
                    const midY = (y1 + y2) / 2;
                    label.style.left = `${midX}px`;
                    label.style.top = `${midY - 10}px`;
                    label.style.transform = 'translate(-50%, -50%)';
                }
            });
        }

        window.addEventListener('load', drawArrows);
        window.addEventListener('resize', drawArrows);
        setTimeout(drawArrows, 100);
    </script>
</body>
</html>
